{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks\n",
    "======\n",
    "\n",
    "Convolutional neural networks (CNNs) are a class of deep neural networks, most commonly used in computer vision applications.\n",
    "\n",
    "Convolutional refers the network pre-processing data for you - traditionally this pre-processing was performed by data scientists. The neural network can learn how to do pre-processing *itself* by applying filters for things such as edge detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1\n",
    "-----\n",
    "\n",
    "In this exercise we will train a CNN to recognise handwritten digits, using the MNIST digit dataset.\n",
    "\n",
    "This is a very common exercise and data set to learn from.\n",
    "\n",
    "Let's start by loading our dataset and setting up our train, validation, and test sets.\n",
    "\n",
    "#### Run the code below to import our required libraries and set up the graphing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras using tensorflow backend\n"
     ]
    }
   ],
   "source": [
    "# Run this!\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "print('keras using %s backend'%keras.backend.backend())\n",
    "import matplotlib.pyplot as graph\n",
    "%matplotlib inline\n",
    "graph.rcParams['figure.figsize'] = (15,5)\n",
    "graph.rcParams[\"font.family\"] = 'DejaVu Sans'\n",
    "graph.rcParams[\"font.size\"] = '12'\n",
    "graph.rcParams['image.cmap'] = 'rainbow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the cell below replace:\n",
    "#### 1. `<addTrainX>` with `train_X`\n",
    "#### 2. `<addTrainY>` with `train_Y`\n",
    "#### 3. `<addValidX>` with `valid_X`\n",
    "#### 4. `<addValidY>` with `valid_Y`\n",
    "#### 5. `<addTextX>` with `test_X`\n",
    "#### 6. `<addTextY>` with `test_Y`\n",
    "#### and then __run the code__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-9420be63e800>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-9420be63e800>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    <addTrainX> = mnist.load_data()[0][0][:6400].astype('float32')\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Here we import the dataset, and split it into the training, validation, and test sets.\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# This is our training data, with 6400 samples.\n",
    "###\n",
    "# REPLACE <addTrainX> WITH train_X AND <addTrainY> WITH train_Y\n",
    "###\n",
    "train_X = mnist.load_data()[0][0][:6400].astype('float32')\n",
    "train_Y = mnist.load_data()[0][1][:6400]\n",
    "###\n",
    "\n",
    "# This is our validation data, with 1600 samples.\n",
    "###\n",
    "# REPLACE <addValidX> WITH valid_X AND <addValidY> WITH valid_Y\n",
    "###\n",
    "valid_X = mnist.load_data()[1][0][:1600].astype('float32')\n",
    "<addValidY> = mnist.load_data()[1][1][:1600]\n",
    "###\n",
    "\n",
    "# This is our test data, with 2000 samples.\n",
    "###\n",
    "# REPLACE <addTextX> WITH test_X AND <addTextY> WITH test_Y\n",
    "###\n",
    "<addTextX> = mnist.load_data()[1][0][-2000:].astype('float32')\n",
    "<addTextY> = mnist.load_data()[1][1][-2000:]\n",
    "###\n",
    "\n",
    "print('train_X:', train_X.shape, end = '')\n",
    "print(', train_Y:', train_Y.shape)\n",
    "print('valid_X:', valid_X.shape, end = '')\n",
    "print(', valid_Y:', valid_Y.shape)\n",
    "print('test_X:', test_X.shape, end = '')\n",
    "print(', test_Y:', test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 6400 training samples, 1600 validation samples, and 2000 test samples.\n",
    "\n",
    "Each sample is an greyscale image - 28 pixels wide and 28 pixels high. Each pixel is really a number from 0 to 255 - 0 being fully black, 255 being fully white. When we graph the 28x28 numbers, we can see the image.\n",
    "\n",
    "Let's have a look at one of our samples.\n",
    "\n",
    "#### Replace `<addSample>` with `train_X[0]` (you can change 0 to any number between 0 and 6400 if you like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# REPLACE THE <addSample> BELOW WITH train_X[0] OR ANOTHER SAMPLE e.g. train_X[1] or train_X[2]\n",
    "###\n",
    "graph.imshow(<addSample>, cmap = 'gray', interpolation = 'nearest')\n",
    "###\n",
    "\n",
    "graph.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2\n",
    "---\n",
    "\n",
    "The neural network will use the 28x28 values of each image to predict what each image represents.\n",
    "\n",
    "As each value is between 0 and 255, we'll scale the values down by dividing by 255 (this makes it faster for the Neural Network to train).\n",
    "\n",
    "We need to reshape our data to get it working well with our neural network. \n",
    "\n",
    "### In the cell below replace:\n",
    "#### 1. `<addRehape>` with `reshape`\n",
    "#### 2. `<completeCalculation>` with `/255`\n",
    "#### and then __run the code__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First off, let's reshape our X sets so that they fit the convolutional layers.\n",
    "\n",
    "# This gets the image dimensions - 28\n",
    "dim = train_X[0].shape[0]\n",
    "\n",
    "###\n",
    "# REPLACE THE <addRehape> BELOW WITH reshape\n",
    "###\n",
    "train_X = train_X.<addRehape>(train_X.shape[0], dim, dim, 1)\n",
    "valid_X = valid_X.<addRehape>(valid_X.shape[0], dim, dim, 1)\n",
    "test_X = test_X.<addRehape>(test_X.shape[0], dim, dim, 1)\n",
    "###\n",
    "\n",
    "# Next up - feature scaling.\n",
    "# We scale the values so they are between 0 and 1, instead of 0 and 255.\n",
    "\n",
    "###\n",
    "# REPLACE THE <completeCalculation> BELOW WITH /255\n",
    "###\n",
    "train_X = train_X <completeCalculation>\n",
    "valid_X = valid_X <completeCalculation>\n",
    "test_X = test_X <completeCalculation>\n",
    "###\n",
    "\n",
    "\n",
    "# Now we print the label for the first example\n",
    "print(train_Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:  \n",
    "`5`\n",
    "\n",
    "The label is a number - the number we see when we view the image.\n",
    "\n",
    "We need represent this number as a one-hot vector, so the neural network knows it is a category.\n",
    "\n",
    "Keras can convert these labels into one-hot vectors easily with the function - `to_categorical`\n",
    "\n",
    "#### Replace `<addCategorical>` with `to_categorical`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# REPLACE THE <addCategorical> BELOW WITH to_categorical\n",
    "###\n",
    "train_Y = keras.utils.<addCategorical>(train_Y, 10)\n",
    "valid_Y = keras.utils.<addCategorical>(valid_Y, 10)\n",
    "test_Y = keras.utils.<addCategorical>(test_Y, 10)\n",
    "###\n",
    "\n",
    "# 10 being the number of categories (numbers 0 to 9)\n",
    "\n",
    "print(train_Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:  \n",
    "`[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]`\n",
    "\n",
    "Step 3\n",
    "-----\n",
    "\n",
    "All ready! Time to build another neural network.\n",
    "\n",
    "#### Replace `<addSequential>` with `Sequential()` and run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets a randomisation seed for replicatability.\n",
    "np.random.seed(6)\n",
    "\n",
    "###\n",
    "# REPLACE THE <addSequential> BELOW WITH Sequential() (don't forget the () )\n",
    "###\n",
    "model = <addSequential>\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __Convolutional__ in Convolutional Neural Networks refers the pre-processing the network can do itself.\n",
    "\n",
    "#### Replace `<addConv2d>` with `Conv2D`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# REPLACE THE <addConv2D> BELOW WITH Conv2D\n",
    "###\n",
    "model.add(<addConv2D>(28, kernel_size = (3, 3), activation = 'relu', input_shape = (dim, dim, 1)))\n",
    "model.add(<addConv2D>(56, (3, 3), activation = 'relu'))\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we'll:\n",
    "* Add pooling layers.\n",
    "* Apply dropout.\n",
    "* Flatten the data to a vector (the output of step 2 is a vector).\n",
    "\n",
    "### In the cell below replace:\n",
    "#### 1. `<addMaxPooling2D>` with `MaxPooling2D`\n",
    "#### 2. `<addDropout>` with `Dropout`\n",
    "#### 3. `<addFlatten>` with `Flatten()`\n",
    "\n",
    "#### and then __run the code__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling layers help speed up training time and make features it detects more robust.\n",
    "# They act by downsampling the data - reducing the data size and complexity.\n",
    "\n",
    "###\n",
    "# REPLACE THE <addMaxPooling2D> BELOW WITH MaxPooling2D\n",
    "###\n",
    "model.add(<addMaxPooling2D>(pool_size = (2, 2)))\n",
    "###\n",
    "\n",
    "# Dropout is a technique to help prevent overfitting\n",
    "# It makes nodes 'dropout' - turning them off randomly.\n",
    "\n",
    "###\n",
    "# REPLACE THE <addDropout> BELOW WITH Dropout\n",
    "###\n",
    "model.add(<addDropout>(0.125))\n",
    "###\n",
    "\n",
    "\n",
    "###\n",
    "# REPLACE THE <addFlatten> BELOW WITH Flatten()\n",
    "###\n",
    "model.add(<addFlatten>)\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace `<updateHere>` with 10 and run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layers perform classification - we have extracted the features with the convolutional pre-processing\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# More dropout!\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Next is our output layer\n",
    "# Softmax outputs the probability for each category\n",
    "###\n",
    "# REPLACE <updateHere> BELOW WITH 10, THE NUMBER OF CLASSES (DIGITS 0 TO 9)\n",
    "###\n",
    "model.add(Dense(<updateHere>, activation=tf.nn.softmax))\n",
    "###\n",
    "\n",
    "# And finally, we compile.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adamax', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4\n",
    "-----\n",
    "\n",
    "Let's train it!\n",
    "\n",
    "### In the cell below replace:\n",
    "#### 1. `<addTrainX>` with `train_X `\n",
    "#### 2. `<addTrainY>` with `train_Y`\n",
    "#### 3. `<addValidX>` with `valid_X`\n",
    "#### 4. `<addValidY>` with `valid_Y`\n",
    "#### 5. `<addEvaluate>` with `evaluate`\n",
    "\n",
    "#### and then __run the code__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# REPLACE THE <addTrainX> WITH train_X, <addTrainY> WITH train_Y, <addValidX> WITH valid_X, AND <addValidY> WITH valid_Y\n",
    "###\n",
    "training_stats = model.fit(<addTrainX>, <addTrainY>, batch_size = 128, epochs = 12, verbose = 1, validation_data = (<addValidX>, <addValidY>))\n",
    "###\n",
    "\n",
    "###\n",
    "# REPLACE THE <addEvaluate> BELOW WITH evaluate\n",
    "###\n",
    "evaluation = model.<addEvaluate>(test_X, test_Y, verbose=0)\n",
    "###\n",
    "\n",
    "print('Test Set Evaluation: loss = %0.6f, accuracy = %0.2f' %(evaluation[0], 100 * evaluation[1]))\n",
    "\n",
    "# We can plot our training statistics to see how it developed over time\n",
    "accuracy, = graph.plot(training_stats.history['acc'], label = 'Accuracy')\n",
    "training_loss, = graph.plot(training_stats.history['loss'], label = 'Training Loss')\n",
    "graph.legend(handles = [accuracy, training_loss])\n",
    "loss = np.array(training_stats.history['loss'])\n",
    "xp = np.linspace(0,loss.shape[0],10 * loss.shape[0])\n",
    "graph.plot(xp, np.full(xp.shape, 1), c = 'k', linestyle = ':', alpha = 0.5)\n",
    "graph.plot(xp, np.full(xp.shape, 0), c = 'k', linestyle = ':', alpha = 0.5)\n",
    "graph.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "Let's test it on a new sample that it hasn't seen, and see how it classifies it!\n",
    "\n",
    "#### Replace `<addNumber>` with any number between 0 and 1999, then run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# REPLACE THE <addNumber> WITH ANY NUMBER BETWEEN 0 AND 1999\n",
    "###\n",
    "sample = test_X[<addNumber>].reshape(dim, dim)\n",
    "###\n",
    "\n",
    "graph.imshow(sample, cmap = 'gray', interpolation = 'nearest')\n",
    "graph.show()\n",
    "\n",
    "prediction = model.predict(sample.reshape(1, dim, dim, 1))\n",
    "print('prediction: %i' %(np.argmax(prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the prediction? Does it look right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "------\n",
    "\n",
    "Congratulations! We've built a convolutional neural network that is able to recognise handwritten digits with very high accuracy.\n",
    "\n",
    "CNN's are very complex - you're not expected to understand everything (or most things) we covered here. They take a lot of time and practise to properly understand each aspect of them.\n",
    "\n",
    "Here we used:  \n",
    "* __Feature scaling__ - reducing the range of the values. This helps improve training time.\n",
    "* __Convolutional layers__ - network layers that pre-process the data for us. These apply filters to extract features for the neural network to analyze.\n",
    "* __Pooling layers__ - part of the Convolutional layers. They apply filters downsample the data - extracting features.\n",
    "* __Dropout__ - a regularization technique to help prevent overfitting.\n",
    "* __Dense layers__ - neural network layers which perform classification on the features extracted by the convolutional layers and downsampled by the pooling layers.\n",
    "* __Softmax__ - an activation function which outputs the probability for each category."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
